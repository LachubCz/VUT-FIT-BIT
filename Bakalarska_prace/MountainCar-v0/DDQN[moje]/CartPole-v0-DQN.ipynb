{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque                # Trida pro ukladani stavu\n",
    "from __future__ import division              # Deleni realnych cisel (kvuli verzi Pythonu 2.6)\n",
    "\n",
    "import numpy as np                           # Knihovna pro matematicke operace\n",
    "import random                                # Knihovna pro nahodny vyber samplu z pameti\n",
    "import sys                                   # Pro pripojeni knihovny Open AI Gym\n",
    "sys.path.append('/home/xbucha02/libraries')  # Pripojeni knihovny Open AI Gym\n",
    "import gym                                   # Knihovna Open AI Gym\n",
    "#from gym import wrappers                    # Nahravani zaznamu\n",
    "env = gym.make('Acrobot-v1')             # Konkretni hra z Open AI Gym\n",
    "actionCount = env.action_space.n             # Pocet vstupu do prostredi\n",
    "stateSize = env.observation_space.shape[0]   # Pocet vystupu z prostredi\n",
    "#env = wrappers.Monitor(env, '/home/lachubcz/tmp/cartpole-experiment-1', force=True) #Nahravani zaznamu\n",
    "\n",
    "gpuMemoryUsage=0.99                            # Vyuziti pameti GPU\n",
    "import tensorflow as tf                     # Knihovna TensorFlow\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpuMemoryUsage\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Parametry\n",
    "observetime = 500                          # Delka pozorovani\n",
    "episodes = 1500                            # Pocet epizod\n",
    "games = 100                                # Pocet her\n",
    "trainingAfterSucces = 100                  # Pocet trenovani na uspesnem datasetu\n",
    "scores = []                                # Pole pro ulozeni vysledku na analyzu\n",
    "episodesList = []                          # Pole pro ulozeni cisel epizod na analyzu\n",
    "bestScore = 199                            # Promenna pro ukladani nejlepsiho prubezneho vysledku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    \n",
    "HUBER_LOSS_DELTA = 1.0\n",
    "    \n",
    "def huber_loss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "\n",
    "    cond = K.abs(err) < HUBER_LOSS_DELTA\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = HUBER_LOSS_DELTA * (K.abs(err) - 0.5 * HUBER_LOSS_DELTA)\n",
    "\n",
    "    loss = tf.where(cond, L2, L1)   # Keras does not cover where function in tensorflow :-(\n",
    "\n",
    "    return K.mean(loss)\n",
    "\n",
    "class Memory:   # stored as ( s, a, r, s_ )\n",
    "    samples = []\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)        \n",
    "\n",
    "        if len(self.samples) > self.capacity:\n",
    "            self.samples.pop(0)\n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)\n",
    "\n",
    "    def isFull(self):\n",
    "        return len(self.samples) >= self.capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.startEpsilon = 1                           # Pravdepodobnost konani nahodneho tahu na zacatku\n",
    "        self.endEpsilon = 0.01                          # Pravdepodobnost konani nahodneho tahu na konci\n",
    "        self.currentEpsilon = self.startEpsilon         # Soucasna pravdepodobnost konani nahodneho tahu\n",
    "        self.epsilonDiminution = 0.001                  # Hodnota snizovani epsilonu\n",
    "        self.gamma = 0.99                               # Discount faktor\n",
    "        self.minibatchSize = 64                         # Velikost minibatche\n",
    "        self.actionCount = env.action_space.n           # Pocet vstupu do prostredi\n",
    "        self.stateSize = env.observation_space.shape[0] # Pocet vystupu z prostredi\n",
    "        self.learningRate = 0.00025                     # Learning rate\n",
    "        self.fractionUpdate = 0.125\n",
    "        self.memorySize = 100000                        # Velikost Replay memory\n",
    "        self.updateTargetNetFreq = 1000\n",
    "        self.primaryMemory = Memory(self.memorySize)#deque(maxlen=self.memorySize)\n",
    "        self.steps = 0\n",
    "        \n",
    "        self.net = self.getNN(env)\n",
    "        self.netTarget = self.getNN(env)\n",
    "        self.updateTargetNet()\n",
    "\n",
    "    def getNN(self, env):\n",
    "        net = Sequential()\n",
    "        net.add(Dense(64, activation=\"relu\", input_shape=(2,) + env.observation_space.shape ))\n",
    "        net.add(Flatten())\n",
    "        #net.add(Dense(16, activation=\"relu\"))\n",
    "        \n",
    "        net.add(Dense(self.actionCount, activation=\"linear\"))\n",
    "\n",
    "        net.summary()\n",
    "\n",
    "        net.compile(loss=huber_loss, optimizer=optimizers.RMSprop(lr=self.learningRate), metrics=['accuracy'])\n",
    "\n",
    "        return net\n",
    "    \n",
    "    def updateTargetNet(self):\n",
    "        self.netTarget.set_weights(self.net.get_weights())\n",
    "    \n",
    "    def updateTargetNetPartially(self):\n",
    "        weights = self.net.get_weights()\n",
    "        weightsTarget = self.netTarget.get_weights()\n",
    "        \n",
    "        for i in range(len(weightsTarget)):\n",
    "            weightsTarget[i] = weights[i] * self.fractionUpdate + weightsTarget[i] * (1 - self.fractionUpdate)\n",
    "            \n",
    "        self.netTarget.set_weights(weightsTarget)\n",
    "        \n",
    "    def rememberPrimMem(self, sample):\n",
    "        self.primaryMemory.add(sample)\n",
    "        if self.steps % self.updateTargetNetFreq == 0:\n",
    "            self.updateTargetNet()\n",
    "           \n",
    "    def epsilonActulization(self):\n",
    "        self.currentEpsilon = self.endEpsilon + (self.startEpsilon - self.endEpsilon) * math.exp(-self.epsilonDiminution * self.steps)\n",
    "    \n",
    "    def getActionWE(self, state):\n",
    "        if np.random.rand() <= self.currentEpsilon:\n",
    "            return np.random.randint(0, self.actionCount, size=1)[0]\n",
    "        else:\n",
    "            Q = self.net.predict(state)\n",
    "            return np.argmax(Q)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        Q = self.net.predict(state)\n",
    "        return np.argmax(Q)\n",
    "        \n",
    "    def resetEpsilon(self):\n",
    "        self.currentEpsilon = self.startEpsilon\n",
    "    \n",
    "    def trainDQN(self, typeOfMem):\n",
    "        if len(self.primaryMemory) >= self.minibatchSize:\n",
    "            minibatch = random.sample(self.primaryMemory, self.minibatchSize) #z D vybere pocet mb_size samplu\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        for i in range(0, self.minibatchSize):\n",
    "            state = minibatch[i][0]\n",
    "            action = minibatch[i][1]\n",
    "            reward = minibatch[i][2]\n",
    "            nextState = minibatch[i][3]\n",
    "            done = minibatch[i][4]\n",
    "\n",
    "            target_f = self.net.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target_f[0][action] = reward\n",
    "            else:\n",
    "                aNet = self.net.predict(nextState)[0]\n",
    "\n",
    "                target_f[0][action] = reward + self.gamma * np.max(aNet)\n",
    "\n",
    "            self.net.fit(state, target_f, epochs=1, verbose=0)\n",
    "                \n",
    "    def trainDDQN(self):\n",
    "        if len(self.primaryMemory) >= self.minibatchSize:\n",
    "            minibatch = random.sample(self.primaryMemory, self.minibatchSize) #z D vybere pocet mb_size samplu\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        for i in range(0, self.minibatchSize):\n",
    "            state = minibatch[i][0]\n",
    "            action = minibatch[i][1]\n",
    "            reward = minibatch[i][2]\n",
    "            nextState = minibatch[i][3]\n",
    "            done = minibatch[i][4]\n",
    "            \n",
    "            target_f = self.net.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target_f[0][action] = reward\n",
    "            else:\n",
    "                aNet = self.net.predict(nextState)[0]\n",
    "                tNet = self.netTarget.predict(nextState)[0]\n",
    "                target_f[0][action] = reward + self.gamma * tNet[np.argmax(aNet)]\n",
    "            \n",
    "            self.net.fit(state, target_f, epochs=1, verbose=0)\n",
    "    \n",
    "    def loadNN(self, name):\n",
    "        self.net.load_weights(name)\n",
    "\n",
    "    def saveNN(self, name):\n",
    "        self.net.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 2, 64)             448       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 835.0\n",
      "Trainable params: 835.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 2, 64)             448       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 835.0\n",
      "Trainable params: 835.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'Memory' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-14cdaa49c21b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrememberPrimMem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnextState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ulozeni do primarni pameti\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainDDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#trenovani na primarni pameti\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnextState\u001b[0m \u001b[1;31m#zmena stavu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-9294577e8c1d>\u001b[0m in \u001b[0;36mtrainDDQN\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrainDDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimaryMemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminibatchSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimaryMemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminibatchSize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#z D vybere pocet mb_size samplu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Memory' has no len()"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "agent = Agent(env) #vytvoreni agenta\n",
    "    \n",
    "#agent.loadNN(\"./DDQN-MountainCar-v0.h5\") #nacteni NN\n",
    "#agent.updateTargetNet() #nacteni NN do netTarget\n",
    "\n",
    "for eps in range (episodes):\n",
    "    state = env.reset() #resetovani prostredi\n",
    "    state = np.reshape(state, [1, stateSize]) #formatovani\n",
    "\n",
    "    state1 = copy.copy(state)\n",
    "    \n",
    "    state = np.concatenate((state, state1))\n",
    "\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    agent.epsilonActulization() #aktualizace epsilon\n",
    "    \n",
    "    #env.render()\n",
    "\n",
    "    for t in range(observetime):\n",
    "\n",
    "        action = agent.getActionWE(state) #ziskani akce\n",
    "\n",
    "        newState, reward, done, info = env.step(action) #provedeni akce\n",
    "        \n",
    "        nextState = np.reshape(newState, [1, stateSize]) #formatovani\n",
    "        \n",
    "        temp = copy.copy(nextState)\n",
    "        \n",
    "        nextState = np.concatenate((nextState, state1))\n",
    "\n",
    "        state1 = copy.copy(temp)\n",
    "        \n",
    "        nextState = np.expand_dims(nextState, axis=0)\n",
    "        \n",
    "        agent.rememberPrimMem((state, action, reward, nextState, done)) #ulozeni do primarni pameti\n",
    "        \n",
    "        agent.trainDDQN() #trenovani na primarni pameti\n",
    "        \n",
    "        state = nextState #zmena stavu\n",
    "        \n",
    "        if done: #konec epizody\n",
    "            print(\"Episode: {}/{}, epsilon: {:.2}, score: {}\".format(eps, episodes, agent.currentEpsilon, t))\n",
    "           \n",
    "            #agent.trainDDQN() #trenovani po hre\n",
    "            \n",
    "            scores.append(t) #ulozeni aktualniho skore\n",
    "            episodesList.append(eps) #ulozeni aktualniho cisla epizody\n",
    "            \n",
    "            agent.updateTargetNet() #aktualizace target site\n",
    "            \n",
    "            agent.saveNN(\"./DDQN-CartPole-v0-{}.h5\" .format(eps)) #ulozeni site\n",
    "            \n",
    "            break\n",
    "            \n",
    "agent.saveNN(\"./DDQN-CartPole-v0.h5\") #ulozeni site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VÃ½sledky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def analysis(scores, episodesList):\n",
    "    score1 = copy.copy(scores)\n",
    "    score2 = copy.copy(scores)\n",
    "    score3 = copy.copy(scores)\n",
    "\n",
    "    for i in range (len(scores)):\n",
    "        if i > 1 and i < (len(scores)-2):\n",
    "            score1[i] = (scores[i - 2] + scores[i - 1] + scores[i] + scores[i + 1] + scores[i + 2])/5\n",
    "\n",
    "    for i in range (len(scores)):\n",
    "        if i > 4 and i < (len(scores)-5):\n",
    "            score2[i] += scores[i - 5] + scores[i - 4] + scores[i - 3] + scores[i - 2] + scores[i - 1]\n",
    "            score2[i] += scores[i + 5] + scores[i + 4] + scores[i + 3] + scores[i + 2] + scores[i + 1]\n",
    "            score2[i] = score2[i]/11\n",
    "\n",
    "    for i in range (len(scores)):\n",
    "        if i > 49 and i < (len(scores) - 50):\n",
    "            for e in range (1,50):\n",
    "                score3[i] += scores[i - e] + scores[i + e] \n",
    "            score3[i] = score3[i]/101      \n",
    "\n",
    "    plt.plot(episodesList, scores, 'ro')\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Vysledky\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(scores)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(score1)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku (filtr - prumer 5-ti prvku)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(score2)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku  (filtr - prumer 11-cti prvku)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(score3)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku  (filtr - prumer 101 prvku)\")\n",
    "    plt.show()\n",
    "    \n",
    "analysis(scores, episodesList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.loadNN(\"./DDQN-CartPole-v0.h5\")\n",
    "observation = env.reset()\n",
    "scores = []                                # Vycisteni pole pro ulozeni vysledku na analyzu\n",
    "episodesList = []                          # Vycisteni pole pro ulozeni cisel epizod na analyzu\n",
    "\n",
    "for g in range (games):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, stateSize])\n",
    "    \n",
    "    state1 = copy.copy(state)\n",
    "    state2 = copy.copy(state)\n",
    "    state3 = copy.copy(state)\n",
    "    \n",
    "    state = np.concatenate((state, state1))\n",
    "    state = np.concatenate((state, state2))\n",
    "    state = np.concatenate((state, state3))\n",
    "\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    totalReward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        \n",
    "        action = agent.getAction(state)\n",
    "        newState, reward, done, info = env.step(action)\n",
    "\n",
    "        nextState = np.reshape(newState, [1, stateSize])\n",
    "\n",
    "        temp = copy.copy(nextState)\n",
    "        \n",
    "        nextState = np.concatenate((nextState, state1))\n",
    "        nextState = np.concatenate((nextState, state2))\n",
    "        nextState = np.concatenate((nextState, state3))\n",
    "        \n",
    "        state3 = copy.copy(state2)\n",
    "        state2 = copy.copy(state1)\n",
    "        state1 = copy.copy(temp)\n",
    "        \n",
    "        nextState = np.expand_dims(nextState, axis=0)\n",
    "        \n",
    "        state = nextState\n",
    "\n",
    "        totalReward += reward\n",
    "        \n",
    "    scores.append(totalReward) #ulozeni aktualniho skore\n",
    "    episodesList.append(g) #ulozeni aktualniho cisla epizody\n",
    "\n",
    "    print('Game {}/{}, score: {}'.format(g, games, totalReward))\n",
    "\n",
    "analysis(scores, episodesList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
