{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque               # Ukladani stavu\n",
    "from __future__ import division             # Deleni realnych cisel (kvuli verzi Pythonu 2.6)\n",
    "import sys                                  # Pro navazani knihovny Open AI Gym\n",
    "import numpy as np                          # Knihovna pro operace s maticemi\n",
    "sys.path.append('/home/xbucha02/libraries') # Adresa knihovny Open AI Gym\n",
    "import gym                                  # Knihovna Open AI Gym\n",
    "#from gym import wrappers                    # Pomucka na nahravani\n",
    "env = gym.make('MountainCar-v0')            # Konkretni hra z Open AI Gym\n",
    "#env = wrappers.Monitor(env, '/home/lachubcz/tmp/cartpole-experiment-1', force=True) #Nahravani zaznamu\n",
    "#gpuMemoryUsage=1                            # Vyuziti pameti GPU\n",
    "#import tensorflow as tf                     # Knihovna TensorFlow pro sdileni GPU\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = gpuMemoryUsage\n",
    "#set_session(tf.Session(config=config))\n",
    "import random\n",
    "\n",
    "# Parametry\n",
    "observetime = 500                          # Pocet framu kazde hry\n",
    "episodes = 1000                             # Pocet epizod\n",
    "startEpsilon = 0.95                         # Pravdepodobnost konani nahodneho tahu na zacatku\n",
    "endEpsilon = 0.01                           # Pravdepodobnost konani nahodneho tahu na konci\n",
    "currentEpsilon = startEpsilon              # Soucasna pravdepodobnost konani nahodneho tahu\n",
    "epsilonDiminution = 0.995 #(startEpsilon - endEpsilon)/(episodes * 5)                 # Hodnota snizovani epsilonu\n",
    "gamma = 0.9                               # Discount faktor\n",
    "minibatchSize = 32                         # Velikost minibatche\n",
    "actionCount = env.action_space.n           # Pocet vstupu do prostredi\n",
    "stateSize = env.observation_space.shape[0] # Pocet vystupu z prostredi\n",
    "learningRate = 0.005                       # Learning rate\n",
    "memorySize = 2000                         # Velikost Replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "def _huber_loss(target, prediction):\n",
    "    # sqrt(1+error^2)-1\n",
    "    error = prediction - target\n",
    "    return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
    "\n",
    "def getDQN(actionCount,stateSize):\n",
    "    net = Sequential()\n",
    "    net.add(Dense(24, activation=\"relu\", input_dim=stateSize))\n",
    "    net.add(Dense(48, activation=\"relu\"))\n",
    "    net.add(Dense(24, activation=\"relu\"))\n",
    "    net.add(Dense(actionCount, activation=\"linear\"))\n",
    "    \n",
    "    net.summary()\n",
    "    \n",
    "    net.compile(loss=losses.mean_squared_error, optimizer=optimizers.Adam(lr=learningRate), metrics=['accuracy'])\n",
    "    \n",
    "    return net\n",
    "\n",
    "def loadDQN(net, name):\n",
    "    net.load_weights(name)\n",
    "\n",
    "def saveDQN(net, name):\n",
    "    net.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmus DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 48)                1200      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 2,523\n",
      "Trainable params: 2,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 48)                1200      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 2,523\n",
      "Trainable params: 2,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode: 0/1000, epsilon: 0.35, score: 199\n"
     ]
    }
   ],
   "source": [
    "net = getDQN(actionCount, stateSize)\n",
    "netTarget = getDQN(actionCount, stateSize)\n",
    "observation = env.reset()                    # Reset prostredi\n",
    "memory = deque(maxlen=memorySize)\n",
    "\n",
    "global scores\n",
    "scores = []\n",
    "global episodes\n",
    "episodesList = []\n",
    "\n",
    "for eps in range (episodes):\n",
    "    #D = deque() #vyprazdneni D\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, stateSize])\n",
    "    #env.render()\n",
    "\n",
    "    for t in range(observetime):\n",
    "\n",
    "        if currentEpsilon > endEpsilon:\n",
    "            currentEpsilon = currentEpsilon * epsilonDiminution\n",
    "            \n",
    "        if np.random.rand() <= currentEpsilon:\n",
    "            action = np.random.randint(0, actionCount, size=1)[0]\n",
    "        else:\n",
    "            Q = net.predict(state)\n",
    "            action = np.argmax(Q)\n",
    "\n",
    "        observation_new, reward, done, info = env.step(int(action))\n",
    "        \n",
    "        next_state = np.reshape(observation_new, [1, stateSize])\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(memory) >= minibatchSize:\n",
    "            minibatch = random.sample(memory, minibatchSize) #z D vybere pocet mb_size samplu\n",
    "\n",
    "            for i in range(0, minibatchSize):\n",
    "\n",
    "                stateMb = minibatch[i][0]\n",
    "                actionMb = minibatch[i][1]\n",
    "                rewardMb = minibatch[i][2]\n",
    "                state_newMb = minibatch[i][3]\n",
    "                doneMb = minibatch[i][4]\n",
    "\n",
    "                target_f = net.predict(stateMb)\n",
    "\n",
    "                if doneMb:\n",
    "                    target_f[0][actionMb] = rewardMb\n",
    "                else:\n",
    "                    aNet = net.predict(state_newMb)[0]\n",
    "                    tNet = netTarget.predict(state_newMb)[0]\n",
    "                    target_f[0][actionMb] = rewardMb + gamma * tNet[np.argmax(aNet)]\n",
    "\n",
    "                net.fit(stateMb, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        weights = net.get_weights()\n",
    "        targetWeights = netTarget.get_weights()\n",
    "        for i in range(len(targetWeights)):\n",
    "            targetWeights[i] = weights[i] * 0.125 + targetWeights[i] * (1 - 0.125)\n",
    "        netTarget.set_weights(targetWeights)\n",
    "        \n",
    "        state = next_state         # Update state\n",
    "        \n",
    "        #if t < 199:\n",
    "        saveDQN(net, \"./DQN-MountainCar-v0-{}.h5\" .format(eps))\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode: {}/{}, epsilon: {:.2}, score: {}\".format(eps, episodes, currentEpsilon, t))\n",
    "            scores.append(t)\n",
    "            episodesList.append(eps)\n",
    "            #netTarget.set_weights(net.get_weights())\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VÃ½sledky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "score1 = copy.copy(scores)\n",
    "score2 = copy.copy(scores)\n",
    "score3 = copy.copy(scores)\n",
    "\n",
    "for i in range (len(scores)):\n",
    "    if i > 1 and i < (len(scores)-2):\n",
    "        score1[i] = (scores[i - 2] + scores[i - 1] + scores[i] + scores[i + 1] + scores[i + 2])/5\n",
    "        \n",
    "for i in range (len(scores)):\n",
    "    if i > 4 and i < (len(scores)-5):\n",
    "        score2[i] += scores[i - 5] + scores[i - 4] + scores[i - 3] + scores[i - 2] + scores[i - 1]\n",
    "        score2[i] += scores[i + 5] + scores[i + 4] + scores[i + 3] + scores[i + 2] + scores[i + 1]\n",
    "        score2[i] = score2[i]/11\n",
    "\n",
    "for i in range (len(scores)):\n",
    "    if i > 49 and i < (len(scores) - 50):\n",
    "        for e in range (1,50):\n",
    "            score3[i] += scores[i - e] + scores[i + e] \n",
    "        score3[i] = score3[i]/101      \n",
    "\n",
    "plt.plot(episodesList, scores, 'ro')\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.title(\"Vysledky\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.title(\"Funkce interpolace vysledku\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(score1)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.title(\"Funkce interpolace vysledku (filtr - prumer 5-ti prvku)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(score2)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.title(\"Funkce interpolace vysledku  (filtr - prumer 11-cti prvku)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(score3)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.title(\"Funkce interpolace vysledku  (filtr - prumer 101 prvku)\")\n",
    "plt.show()\n",
    "\n",
    "saveDQN(net, \"./DQN-MountainCar-v0.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netForPlay = getDQN(actionCount, stateSize)\n",
    "loadDQN(netForPlay, \"./DQN-MountainCar-v0.h5\")\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range (1000):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, stateSize])\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        Q = netForPlay.predict(state)   \n",
    "        action = np.argmax(Q)  \n",
    "        observation_new, reward, done, info = env.step(int(action))\n",
    "\n",
    "        next_state = np.reshape(observation_new, [1, stateSize])\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "    print('{}. Game - score: {}'.format(i, total_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
