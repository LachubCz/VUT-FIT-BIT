{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 20)                100       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 562.0\n",
      "Trainable params: 562\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "0 10.8 12\n",
      "100 53.3 3923\n",
      "200 90.5 8033\n",
      "300 62.8 10000\n",
      "400 46.4 10000\n",
      "500 48.4 10000\n",
      "600 52.7 10000\n",
      "700 62.6 10000\n",
      "800 58.9 10000\n",
      "900 51.8 10000\n",
      "1000 57.5 10000\n",
      "1100 56.9 10000\n",
      "1200 44.2 10000\n",
      "1300 58.2 10000\n",
      "1400 56.8 10000\n",
      "1500 169.4 10000\n",
      "1600 140.5 10000\n",
      "1700 101.4 10000\n",
      "1800 76.8 10000\n",
      "1900 73.9 10000\n",
      "2000 137.1 10000\n",
      "2100 165.2 10000\n",
      "2200 114.6 10000\n",
      "2300 167.5 10000\n",
      "2400 161.1 10000\n",
      "2500 169.0 10000\n",
      "2600 170.6 10000\n",
      "2700 155.2 10000\n",
      "2800 112.2 10000\n",
      "2900 180.9 10000\n",
      "3000 177.0 10000\n",
      "3100 175.5 10000\n",
      "3200 177.8 10000\n",
      "3300 172.6 10000\n",
      "3400 134.7 10000\n",
      "3500 104.3 10000\n",
      "3600 128.9 10000\n",
      "3700 167.9 10000\n",
      "3800 140.0 10000\n",
      "3900 74.2 10000\n",
      "4000 152.8 10000\n",
      "4100 126.6 10000\n",
      "4200 92.1 10000\n",
      "4300 53.3 10000\n",
      "4400 69.3 10000\n",
      "4500 120.9 10000\n",
      "4600 90.8 10000\n",
      "4700 47.5 10000\n",
      "4800 53.4 10000\n",
      "4900 135.0 10000\n",
      "5000 61.3 10000\n",
      "5100 58.2 10000\n",
      "5200 59.4 10000\n",
      "5300 55.0 10000\n",
      "5400 44.1 10000\n",
      "5500 53.8 10000\n",
      "5600 58.5 10000\n",
      "5700 49.3 10000\n",
      "5800 49.8 10000\n",
      "5900 48.3 10000\n",
      "6000 57.3 10000\n",
      "6100 48.4 10000\n",
      "6200 52.4 10000\n",
      "6300 44.5 10000\n",
      "6400 43.9 10000\n",
      "6500 56.1 10000\n",
      "6600 45.6 10000\n",
      "6700 52.7 10000\n",
      "6800 72.0 10000\n",
      "6900 42.7 10000\n",
      "7000 46.6 10000\n",
      "7100 68.8 10000\n",
      "7200 51.5 10000\n",
      "7300 70.6 10000\n",
      "7400 77.1 10000\n",
      "7500 72.8 10000\n",
      "7600 84.7 10000\n",
      "7700 89.3 10000\n",
      "7800 87.9 10000\n",
      "7900 73.6 10000\n",
      "8000 82.7 10000\n",
      "8100 110.2 10000\n",
      "8200 79.0 10000\n",
      "8300 72.0 10000\n",
      "8400 70.4 10000\n",
      "8500 84.9 10000\n",
      "8600 80.5 10000\n",
      "8700 90.7 10000\n",
      "8800 51.3 10000\n",
      "8900 79.4 10000\n",
      "9000 83.6 10000\n",
      "9100 82.0 10000\n",
      "9200 66.5 10000\n",
      "9300 62.5 10000\n",
      "9400 77.0 10000\n",
      "9500 48.0 10000\n",
      "9600 59.5 10000\n",
      "9700 66.3 10000\n",
      "9800 72.6 10000\n",
      "9900 48.6 10000\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import sys                                   # Pro pripojeni knihovny Open AI Gym\n",
    "sys.path.append('/home/xbucha02/libraries')  # Pripojeni knihovny Open AI Gym\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import save_model\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.5\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.replay = deque(maxlen=10000)\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.epsilon = EPSILON\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(20, activation='relu', input_shape=(self.state_dim,)))\n",
    "        self.model.add(Dense(20, activation='relu'))\n",
    "        self.model.add(Dense(self.action_dim, activation='linear'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "\n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        self.replay.append((state, action, reward, next_state, done))\n",
    "        if len(self.replay) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        minbatch = random.sample(self.replay, BATCH_SIZE)\n",
    "        state_batch = np.vstack([i[0] for i in minbatch])\n",
    "        action_batch = [i[1] for i in minbatch]\n",
    "        reward_batch = [i[2] for i in minbatch]\n",
    "        next_state_batch = np.vstack([i[3] for i in minbatch])\n",
    "        y_batch = []\n",
    "#        print(\"Statetr: {}, {}\" .format(state_batch[1], np.shape(next_state_batch[1])))\n",
    "        Q_value_batch = self.model.predict(next_state_batch)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if minbatch[i][4]:\n",
    "                reward = reward_batch[i]\n",
    "            else:\n",
    "                reward = reward_batch[i] + GAMMA * np.max(Q_value_batch[i])\n",
    "            target = [0 for _ in range(self.action_dim)]\n",
    "            target[action_batch[i]] = reward\n",
    "            y_batch.append(target)\n",
    "        #print(\"state_batch:{} y_batch: {}\" .format(state_batch, y_batch))\n",
    "        self.model.train_on_batch(state_batch, y_batch)\n",
    "\n",
    "    def e_action(self, state):\n",
    "        Q_value = self.model.predict(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "        self.epsilon -= (EPSILON - 0.01) / 10000\n",
    "\n",
    "    def action(self, state):\n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def load(self, model):\n",
    "        self.model = load_model(model)\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DQN(env)\n",
    "flag = 0\n",
    "#agent.load('2.h5')\n",
    "\n",
    "for i in range(10000):\n",
    "    state = env.reset()\n",
    "    for s in range(400):\n",
    "        state = np.reshape(state, (1, agent.state_dim))\n",
    "        action = agent.e_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = -1 if done else 0.1\n",
    "        #print(\"State: {}next_state:{}\" .format(state, next_state))\n",
    "        agent.perceive(state, action, reward, next_state, done)\n",
    "        #print(\"State: {}\" .format(state))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        t_reward = 0\n",
    "        for j in range(10):\n",
    "            state = env.reset()\n",
    "            while True:\n",
    "                state = np.reshape(state, (1, agent.state_dim))\n",
    "                action = agent.action(state)\n",
    "                #env.render()\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                t_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "        print(i, t_reward / 10, len(agent.replay))\n",
    "        if t_reward / 10 >= 250:\n",
    "            break\n",
    "    if t_reward / 10 > flag:\n",
    "        flag = t_reward / 10\n",
    "        save_model(agent.model, 'cartpole.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
