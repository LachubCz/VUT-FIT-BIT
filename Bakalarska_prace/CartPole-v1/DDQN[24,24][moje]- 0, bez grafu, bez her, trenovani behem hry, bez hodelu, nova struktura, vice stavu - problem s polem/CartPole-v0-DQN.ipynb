{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque                # Trida pro ukladani stavu\n",
    "from __future__ import division              # Deleni realnych cisel (kvuli verzi Pythonu 2.6)\n",
    "\n",
    "import numpy as np                           # Knihovna pro matematicke operace\n",
    "import random                                # Knihovna pro nahodny vyber samplu z pameti\n",
    "import sys                                   # Pro pripojeni knihovny Open AI Gym\n",
    "sys.path.append('/home/xbucha02/libraries')  # Pripojeni knihovny Open AI Gym\n",
    "import gym                                   # Knihovna Open AI Gym\n",
    "#from gym import wrappers                    # Nahravani zaznamu\n",
    "env = gym.make('CartPole-v1')             # Konkretni hra z Open AI Gym\n",
    "actionCount = env.action_space.n             # Pocet vstupu do prostredi\n",
    "stateSize = env.observation_space.shape[0]   # Pocet vystupu z prostredi\n",
    "#env = wrappers.Monitor(env, '/home/lachubcz/tmp/cartpole-experiment-1', force=True) #Nahravani zaznamu\n",
    "\n",
    "gpuMemoryUsage=1.2                            # Vyuziti pameti GPU\n",
    "import tensorflow as tf                     # Knihovna TensorFlow\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpuMemoryUsage\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Parametry\n",
    "observetime = 500                          # Delka pozorovani\n",
    "episodes = 1500                            # Pocet epizod\n",
    "games = 100                                # Pocet her\n",
    "trainingAfterSucces = 100                  # Pocet trenovani na uspesnem datasetu\n",
    "scores = []                                # Pole pro ulozeni vysledku na analyzu\n",
    "episodesList = []                          # Pole pro ulozeni cisel epizod na analyzu\n",
    "bestScore = 0                              # Promenna pro ukladani nejlepsiho prubezneho vysledku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.startEpsilon = 1                           # Pravdepodobnost konani nahodneho tahu na zacatku\n",
    "        self.endEpsilon = 0.01                          # Pravdepodobnost konani nahodneho tahu na konci\n",
    "        self.currentEpsilon = self.startEpsilon         # Soucasna pravdepodobnost konani nahodneho tahu\n",
    "        self.epsilonDiminution = 0.995                  # Hodnota snizovani epsilonu\n",
    "        self.gamma = 0.95                               # Discount faktor\n",
    "        self.minibatchSize = 32                         # Velikost minibatche\n",
    "        self.actionCount = env.action_space.n           # Pocet vstupu do prostredi\n",
    "        self.stateSize = env.observation_space.shape[0] # Pocet vystupu z prostredi\n",
    "        self.learningRate = 0.005                       # Learning rate\n",
    "        self.fractionUpdate = 0.125\n",
    "        self.memorySize = 2000                          # Velikost Replay memory\n",
    "        self.primaryMemory = deque(maxlen=self.memorySize)\n",
    "        self.secondaryMemory = deque(maxlen=self.memorySize)\n",
    "        \n",
    "        self.net = self.getNN(env)\n",
    "        self.netTarget = self.getNN(env)\n",
    "        self.updateTargetNet()\n",
    "\n",
    "    def getNN(self, env):\n",
    "        net = Sequential()\n",
    "        net.add(Dense(24, activation=\"relu\", input_shape=(2,) + env.observation_space.shape ))\n",
    "        net.add(Flatten())\n",
    "        net.add(Dense(24, activation=\"relu\"))\n",
    "        \n",
    "        net.add(Dense(self.actionCount, activation=\"linear\"))\n",
    "\n",
    "        net.summary()\n",
    "\n",
    "        net.compile(loss=losses.mean_squared_error, optimizer=optimizers.Adam(lr=self.learningRate), metrics=['accuracy'])\n",
    "\n",
    "        return net\n",
    "    \n",
    "    def updateTargetNet(self):\n",
    "        self.netTarget.set_weights(self.net.get_weights())\n",
    "    \n",
    "    def updateTargetNetPartially(self):\n",
    "        weights = self.net.get_weights()\n",
    "        weightsTarget = self.netTarget.get_weights()\n",
    "        \n",
    "        for i in range(len(weightsTarget)):\n",
    "            weightsTarget[i] = weights[i] * self.fractionUpdate + weightsTarget[i] * (1 - self.fractionUpdate)\n",
    "            \n",
    "        self.netTarget.set_weights(weightsTarget)\n",
    "        \n",
    "    def rememberPrimMem(self, state, action, reward, nextState, done):\n",
    "        self.primaryMemory.append((state, action, reward, nextState, done))\n",
    "    \n",
    "    def rememberSecMem(self, state, action, reward, nextState, done):\n",
    "        self.secondaryMemory.append((state, action, reward, nextState, done))\n",
    "        \n",
    "    def clearSecMem(self):\n",
    "        self.secondaryMemory.clear()\n",
    "        \n",
    "    def epsilonActulization(self):\n",
    "        if self.currentEpsilon > self.endEpsilon:\n",
    "            if (self.currentEpsilon * self.epsilonDiminution) > self.endEpsilon:\n",
    "                self.currentEpsilon = self.currentEpsilon * self.epsilonDiminution\n",
    "            else:\n",
    "                self.currentEpsilon = self.endEpsilon\n",
    "    \n",
    "    def getActionWE(self, state):\n",
    "        if np.random.rand() <= self.currentEpsilon:\n",
    "            return np.random.randint(0, self.actionCount, size=1)[0]\n",
    "        else:\n",
    "            Q = self.net.predict(state)\n",
    "            return np.argmax(Q)\n",
    "\n",
    "    def getAction(self, state):\n",
    "        Q = self.net.predict(state)\n",
    "        return np.argmax(Q)\n",
    "    \n",
    "    def resetSecMem(self):\n",
    "        self.secondaryMemory = deque(maxlen=self.memorySize)\n",
    "        \n",
    "    def resetEpsilon(self):\n",
    "        self.currentEpsilon = self.startEpsilon\n",
    "    \n",
    "    def trainDQN(self, typeOfMem):\n",
    "        if typeOfMem == 1:\n",
    "            if len(self.primaryMemory) >= self.minibatchSize:\n",
    "                minibatch = random.sample(self.primaryMemory, self.minibatchSize)\n",
    "            else:\n",
    "                return\n",
    "        elif typeOfMem == 2:\n",
    "            if len(self.secondaryMemory) >= self.minibatchSize:\n",
    "                minibatch = random.sample(self.secondaryMemory, self.minibatchSize)\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        for i in range(0, self.minibatchSize):\n",
    "            state = minibatch[i][0]\n",
    "            action = minibatch[i][1]\n",
    "            reward = minibatch[i][2]\n",
    "            nextState = minibatch[i][3]\n",
    "            done = minibatch[i][4]\n",
    "\n",
    "            target_f = self.net.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target_f[0][action] = reward\n",
    "            else:\n",
    "                aNet = self.net.predict(nextState)[0]\n",
    "\n",
    "                target_f[0][action] = reward + self.gamma * np.max(aNet)\n",
    "\n",
    "            self.net.fit(state, target_f, epochs=1, verbose=0)\n",
    "                \n",
    "    def trainDDQN(self, typeOfMem):\n",
    "        if typeOfMem == 1:\n",
    "            if len(self.primaryMemory) >= self.minibatchSize:\n",
    "                minibatch = random.sample(self.primaryMemory, self.minibatchSize) #z D vybere pocet mb_size samplu\n",
    "            else:\n",
    "                return\n",
    "        elif typeOfMem == 2:\n",
    "            if len(self.secondaryMemory) >= self.minibatchSize:\n",
    "                minibatch = random.sample(self.secondaryMemory, self.minibatchSize) #z D vybere pocet mb_size samplu\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        #states = np.empty(self.minibatchSize)\n",
    "        #print(\"{}\" .format(states))\n",
    "        #target_fs = np.empty(self.minibatchSize)\n",
    "        states = np.vstack([i[0] for i in minibatch])\n",
    "        target_fs = []\n",
    "        \n",
    "        \n",
    "        for i in range(0, self.minibatchSize):\n",
    "            state = minibatch[i][0]\n",
    "            action = minibatch[i][1]\n",
    "            reward = minibatch[i][2]\n",
    "            nextState = minibatch[i][3]\n",
    "            done = minibatch[i][4]\n",
    "            \n",
    "            target_f = self.net.predict(state)\n",
    "\n",
    "            if done:\n",
    "                target_f[0][action] = reward\n",
    "            else:\n",
    "                aNet = self.net.predict(nextState)[0]\n",
    "                tNet = self.netTarget.predict(nextState)[0]\n",
    "                target_f[0][action] = reward + self.gamma * tNet[np.argmax(aNet)]\n",
    "            print(\"{}\".format(state))\n",
    "            print(\"{}\".format(target_f))\n",
    "            #states.append(state)\n",
    "            target_fs.append(target_f)\n",
    "        #self.net.fit(states, target_fs, batch_size=32, epochs=1, verbose=0)\n",
    "        self.net.train_on_batch(states, target_fs)\n",
    "    \n",
    "    def loadNN(self, name):\n",
    "        self.net.load_weights(name)\n",
    "\n",
    "    def saveNN(self, name):\n",
    "        self.net.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 2, 24)             120       \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,346.0\n",
      "Trainable params: 1,346.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_70 (Dense)             (None, 2, 24)             120       \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 1,346.0\n",
      "Trainable params: 1,346.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Episode: 0/1500, epsilon: 0.99, score: 28\n",
      "[[[-0.01238712 -0.41531198 -0.08391186  0.25387644]\n",
      "  [-0.00795745 -0.2214837  -0.08368639 -0.01127338]]]\n",
      "[[ 1.15163529  0.10038289]]\n",
      "[[[-0.04547573 -0.41111561 -0.06662413  0.16073104]\n",
      "  [-0.04113627 -0.21697283 -0.06440499 -0.11095737]]]\n",
      "[[-0.17746359  1.07226169]]\n",
      "[[[-0.04113627 -0.21697283 -0.06440499 -0.11095737]\n",
      "  [-0.0328762  -0.41300362 -0.06845519  0.20251022]]]\n",
      "[[ 1.08932066  0.08144511]]\n",
      "[[[-0.05484572  0.37293592 -0.09127883 -1.09258259]\n",
      "  [-0.05838289  0.17685864 -0.07573754 -0.77706427]]]\n",
      "[[ 1.04438627 -0.07377844]]\n",
      "[[[ 0.0152801   0.1604927  -0.08891627 -0.41575758]\n",
      "  [ 0.00819023  0.35449376 -0.07523955 -0.68383632]]]\n",
      "[[ 0.99219745 -0.06269994]]\n",
      "[[[-0.04480457 -0.19757517 -0.19694693 -0.548591  ]\n",
      "  [-0.03690742 -0.39485743 -0.19291109 -0.20179217]]]\n",
      "[[ 1.04009867  0.01195975]]\n",
      "[[[ 0.01848995 -0.03326396 -0.09723142 -0.15237769]\n",
      "  [ 0.0152801   0.1604927  -0.08891627 -0.41575758]]]\n",
      "[[ 1.03734577 -0.02290906]]\n",
      "[[[-0.03629247  0.1823847  -0.15284589 -0.90653878]\n",
      "  [-0.04380445  0.37559899 -0.12972806 -1.15589173]]]\n",
      "[[ 1.01628089 -0.0983993 ]]\n",
      "[[[ 0.00819023  0.35449376 -0.07523955 -0.68383632]\n",
      "  [-0.00278615  0.54881881 -0.05607176 -0.95838932]]]\n",
      "[[ 1.00926328 -0.09909838]]\n",
      "[[[-0.03264478 -0.01037433 -0.17097667 -0.66553301]\n",
      "  [-0.03629247  0.1823847  -0.15284589 -0.90653878]]]\n",
      "[[ 0.99221283 -0.04708944]]\n",
      "[[[-0.02069336 -0.60914179 -0.07883433  0.51895702]\n",
      "  [-0.01238712 -0.41531198 -0.08391186  0.25387644]]]\n",
      "[[-0.26135486  1.14843774]]\n",
      "[[[-0.04875607 -0.38946493 -0.20791875 -0.32384942]\n",
      "  [-0.04480457 -0.19757517 -0.19694693 -0.548591  ]]]\n",
      "[[ 1.          0.04220917]]\n",
      "[[[-0.05838289  0.17685864 -0.07573754 -0.77706427]\n",
      "  [-0.05800016 -0.01913635 -0.06645359 -0.46419771]]]\n",
      "[[ 0.04035066  1.05570471]]\n",
      "[[[ 0.0152801   0.1604927  -0.08891627 -0.41575758]\n",
      "  [ 0.00819023  0.35449376 -0.07523955 -0.68383632]]]\n",
      "[[ 0.99219745 -0.06269994]]\n",
      "[[[ 0.00487887 -0.22405228 -0.09076554  0.04561759]\n",
      "  [ 0.0132873  -0.42042156 -0.09811664  0.36755487]]]\n",
      "[[ 1.09653831  0.09309933]]\n",
      "[[[-0.01223687 -0.03809853 -0.03514762 -0.04508467]\n",
      "  [-0.01223687 -0.03809853 -0.03514762 -0.04508467]]]\n",
      "[[-0.01281337  1.01788044]]\n",
      "[[[ 0.00819023  0.35449376 -0.07523955 -0.68383632]\n",
      "  [-0.00278615  0.54881881 -0.05607176 -0.95838932]]]\n",
      "[[ 1.00926328 -0.09909838]]\n",
      "[[[  3.97820175e-04  -4.17763390e-01  -8.98531865e-02   3.08339741e-01]\n",
      "  [  4.87886574e-03  -2.24052278e-01  -9.07655384e-02   4.56175944e-02]]]\n",
      "[[-0.17413738  1.08612084]]\n",
      "[[[-0.01238712 -0.41531198 -0.08391186  0.25387644]\n",
      "  [-0.00795745 -0.2214837  -0.08368639 -0.01127338]]]\n",
      "[[ 1.15163529  0.10038289]]\n",
      "[[[-0.00278615  0.54881881 -0.05607176 -0.95838932]\n",
      "  [-0.00984865  0.35312496 -0.04302225 -0.65247554]]]\n",
      "[[ 1.02321959 -0.07339813]]\n",
      "[[[ 0.01782468 -0.22686893 -0.10027898  0.10811709]\n",
      "  [ 0.01848995 -0.03326396 -0.09723142 -0.15237769]]]\n",
      "[[ 1.09572399  0.03931129]]\n",
      "[[[-0.04480457 -0.19757517 -0.19694693 -0.548591  ]\n",
      "  [-0.03690742 -0.39485743 -0.19291109 -0.20179217]]]\n",
      "[[ 1.04009867  0.01195975]]\n",
      "[[[-0.05800016 -0.01913635 -0.06645359 -0.46419771]\n",
      "  [-0.05369804 -0.21510624 -0.06340951 -0.15220373]]]\n",
      "[[-0.01164707  1.03833318]]\n",
      "[[[-0.03629247  0.1823847  -0.15284589 -0.90653878]\n",
      "  [-0.04380445  0.37559899 -0.12972806 -1.15589173]]]\n",
      "[[ 1.01628089 -0.0983993 ]]\n",
      "[[[-0.03285226 -0.20275774 -0.18428733 -0.43118817]\n",
      "  [-0.03264478 -0.01037433 -0.17097667 -0.66553301]]]\n",
      "[[ 1.05799592 -0.008197  ]]\n",
      "[[[-0.04380445  0.37559899 -0.12972806 -1.15589173]\n",
      "  [-0.047387    0.17912745 -0.11313048 -0.82987885]]]\n",
      "[[ 1.04667044 -0.07474919]]\n",
      "[[[-0.0328762  -0.41300362 -0.06845519  0.20251022]\n",
      "  [-0.02069336 -0.60914179 -0.07883433  0.51895702]]]\n",
      "[[-0.25241897  1.07737291]]\n",
      "[[[-0.03690742 -0.39485743 -0.19291109 -0.20179217]\n",
      "  [-0.03285226 -0.20275774 -0.18428733 -0.43118817]]]\n",
      "[[-0.12025535  1.01136172]]\n",
      "[[[-0.05484572  0.37293592 -0.09127883 -1.09258259]\n",
      "  [-0.05838289  0.17685864 -0.07573754 -0.77706427]]]\n",
      "[[ 1.04438627 -0.07377844]]\n",
      "[[[-0.04113627 -0.21697283 -0.06440499 -0.11095737]\n",
      "  [-0.0328762  -0.41300362 -0.06845519  0.20251022]]]\n",
      "[[ 1.08932066  0.08144511]]\n",
      "[[[-0.05369804 -0.21510624 -0.06340951 -0.15220373]\n",
      "  [-0.04547573 -0.41111561 -0.06662413  0.16073104]]]\n",
      "[[-0.17188786  1.00317359]]\n",
      "[[[-0.047387    0.17912745 -0.11313048 -0.82987885]\n",
      "  [-0.05484572  0.37293592 -0.09127883 -1.09258259]]]\n",
      "[[ 0.04672233  1.05777395]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 32 arrays: [array([[ 1.15163529,  0.10038289]], dtype=float32), array([[-0.17746359,  1.07226169]], dtype=float32), array([[ 1.08932066,  0.08144511]], dtype=float32), array([[ 1.04438627, -0.07377844]], dtype=f...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-5a59e4767c84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#konec epizody\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Episode: {}/{}, epsilon: {:.2}, score: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentEpsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainDDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ulozeni aktualniho skore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mepisodesList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ulozeni aktualniho cisla epizody\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-ae97ecc8543b>\u001b[0m in \u001b[0;36mtrainDDQN\u001b[1;34m(self, typeOfMem)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mtarget_fs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m#self.net.fit(states, target_fs, batch_size=32, epochs=1, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_fs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloadNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowGPU\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m    931\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m    932\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowGPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1612\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m             check_batch_axis=True)\n\u001b[0m\u001b[0;32m   1615\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowGPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m                                     exception_prefix='model target')\n\u001b[0m\u001b[0;32m   1300\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1301\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowGPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     72\u001b[0m                                  \u001b[1;34m'the following list of '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                                  \u001b[1;34m' arrays: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                                  '...')\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 32 arrays: [array([[ 1.15163529,  0.10038289]], dtype=float32), array([[-0.17746359,  1.07226169]], dtype=float32), array([[ 1.08932066,  0.08144511]], dtype=float32), array([[ 1.04438627, -0.07377844]], dtype=f..."
     ]
    }
   ],
   "source": [
    "import copy\n",
    "agent = Agent(env) #vytvoreni agenta\n",
    "    \n",
    "#agent.loadNN(\"./DDQN-MountainCar-v0.h5\") #nacteni NN\n",
    "#agent.updateTargetNet() #nacteni NN do netTarget\n",
    "\n",
    "for eps in range (episodes):\n",
    "    state = env.reset() #resetovani prostredi\n",
    "    state = np.reshape(state, [1, stateSize]) #formatovani\n",
    "\n",
    "    state1 = copy.copy(state)\n",
    "    #state2 = copy.copy(state)\n",
    "    #state3 = copy.copy(state)\n",
    "    \n",
    "    state = np.concatenate((state, state1))\n",
    "    #state = np.concatenate((state, state2))\n",
    "    #state = np.concatenate((state, state3))\n",
    "\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    agent.epsilonActulization() #aktualizace epsilon\n",
    "    \n",
    "    #env.render()\n",
    "\n",
    "    for t in range(observetime):\n",
    "\n",
    "        action = agent.getActionWE(state) #ziskani akce\n",
    "\n",
    "        newState, reward, done, info = env.step(action) #provedeni akce\n",
    "        \n",
    "        nextState = np.reshape(newState, [1, stateSize]) #formatovani\n",
    "        \n",
    "        temp = copy.copy(nextState)\n",
    "        \n",
    "        nextState = np.concatenate((nextState, state1))\n",
    "        #nextState = np.concatenate((nextState, state2))\n",
    "        #nextState = np.concatenate((nextState, state3))\n",
    "        \n",
    "        #state3 = copy.copy(state2)\n",
    "        #state2 = copy.copy(state1)\n",
    "        state1 = copy.copy(temp)\n",
    "        \n",
    "        nextState = np.expand_dims(nextState, axis=0)\n",
    "        #print (\"{}\".format(nextState))\n",
    "        agent.rememberPrimMem(state, action, reward, nextState, done) #ulozeni do primarni pameti\n",
    "        agent.rememberPrimMem(state, action, reward, nextState, done) #ulozeni do sekundarni pameti\n",
    "        \n",
    "        #agent.trainDDQN(1) #trenovani na primarni pameti\n",
    "        \n",
    "        state = nextState #zmena stavu\n",
    "        \n",
    "        if done: #konec epizody\n",
    "            print(\"Episode: {}/{}, epsilon: {:.2}, score: {}\".format(eps, episodes, agent.currentEpsilon, t))\n",
    "            agent.trainDDQN(1)\n",
    "            scores.append(t) #ulozeni aktualniho skore\n",
    "            episodesList.append(eps) #ulozeni aktualniho cisla epizody\n",
    "            \n",
    "            agent.updateTargetNet() #aktualizace target site\n",
    "            \n",
    "            #if t > bestScore: #bylo dosazeno njelepsiho skore\n",
    "            #    bestScore = t #nove nejlepsi skore\n",
    "            #    for i in range(trainingAfterSucces): #pocet trenovani\n",
    "            #        agent.trainDDQN(2) #trenovani na sekundarni pameti\n",
    "            #    agent.saveNN(\"./DDQN-CartPole-v0-{}.h5\" .format(eps)) #ulozeni site\n",
    "            #    #agent.resetEpsilon() #resetovani epsilonu\n",
    "            \n",
    "            #agent.resetSecMem() #vycisteni sekundarni pameti\n",
    "            \n",
    "            break\n",
    "            \n",
    "agent.saveNN(\"./DDQN-CartPole-v0.h5\") #ulozeni site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VÃ½sledky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def analysis(scores, episodesList):\n",
    "    score1 = copy.copy(scores)\n",
    "    score2 = copy.copy(scores)\n",
    "    score3 = copy.copy(scores)\n",
    "\n",
    "    for i in range (len(scores)):\n",
    "        if i > 1 and i < (len(scores)-2):\n",
    "            score1[i] = (scores[i - 2] + scores[i - 1] + scores[i] + scores[i + 1] + scores[i + 2])/5\n",
    "\n",
    "    for i in range (len(scores)):\n",
    "        if i > 4 and i < (len(scores)-5):\n",
    "            score2[i] += scores[i - 5] + scores[i - 4] + scores[i - 3] + scores[i - 2] + scores[i - 1]\n",
    "            score2[i] += scores[i + 5] + scores[i + 4] + scores[i + 3] + scores[i + 2] + scores[i + 1]\n",
    "            score2[i] = score2[i]/11\n",
    "\n",
    "    for i in range (len(scores)):\n",
    "        if i > 49 and i < (len(scores) - 50):\n",
    "            for e in range (1,50):\n",
    "                score3[i] += scores[i - e] + scores[i + e] \n",
    "            score3[i] = score3[i]/101      \n",
    "\n",
    "    plt.plot(episodesList, scores, 'ro')\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Vysledky\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(scores)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(score1)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku (filtr - prumer 5-ti prvku)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(score2)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku  (filtr - prumer 11-cti prvku)\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(score3)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.title(\"Funkce interpolace vysledku  (filtr - prumer 101 prvku)\")\n",
    "    plt.show()\n",
    "    \n",
    "analysis(scores, episodesList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.loadNN(\"./DDQN-CartPole-v0.h5\")\n",
    "observation = env.reset()\n",
    "scores = []                                # Vycisteni pole pro ulozeni vysledku na analyzu\n",
    "episodesList = []                          # Vycisteni pole pro ulozeni cisel epizod na analyzu\n",
    "\n",
    "for g in range (games):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, stateSize])\n",
    "    \n",
    "    state1 = copy.copy(state)\n",
    "    state2 = copy.copy(state)\n",
    "    state3 = copy.copy(state)\n",
    "    \n",
    "    state = np.concatenate((state, state1))\n",
    "    state = np.concatenate((state, state2))\n",
    "    state = np.concatenate((state, state3))\n",
    "\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    \n",
    "    totalReward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        \n",
    "        action = agent.getAction(state)\n",
    "        newState, reward, done, info = env.step(action)\n",
    "\n",
    "        nextState = np.reshape(newState, [1, stateSize])\n",
    "\n",
    "        temp = copy.copy(nextState)\n",
    "        \n",
    "        nextState = np.concatenate((nextState, state1))\n",
    "        nextState = np.concatenate((nextState, state2))\n",
    "        nextState = np.concatenate((nextState, state3))\n",
    "        \n",
    "        state3 = copy.copy(state2)\n",
    "        state2 = copy.copy(state1)\n",
    "        state1 = copy.copy(temp)\n",
    "        \n",
    "        nextState = np.expand_dims(nextState, axis=0)\n",
    "        \n",
    "        state = nextState\n",
    "\n",
    "        totalReward += reward\n",
    "        \n",
    "    scores.append(totalReward) #ulozeni aktualniho skore\n",
    "    episodesList.append(g) #ulozeni aktualniho cisla epizody\n",
    "\n",
    "    print('Game {}/{}, score: {}'.format(g, games, totalReward))\n",
    "\n",
    "analysis(scores, episodesList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
