Učení agentů posilovaného učení je časově náročná operace. Tato práce se zabývá zkoumáním hyperparametrů metody DQN a jejich vlivem na efektivitu učení a tedy i rychlost. Nahrazením prostředí, které na vstup neuronové sítě dávají místo obrazu prostředí, vektor číšel popisující jeho stav, jsem dosáhl snížení doby trénování z několika dní na několik minut. Tato změna mi umožnila efektivně testovat hyperparametry a zkoumat jejich vliv na učení.


Má práce se zabývá algoritmem zpětnovazebného učení DQN, jeho cílem je vytvořit agenta, který se v libovolném prostředí bude schopen naučit inteligentnímu chovaní podle reakcí okolí ve formě odměn, jde tedy o formu umělé inteligence. Historie má několik významných milníků, které jsou spjaty s uvedením agentů, kteří dokázali plnit konkrétní úlohu v konkrétním prostředí. Pravděpodobně nejznámnějším takovým milníkem je vítězství počítače Deep Blue nad Garri Kasparovem v šachové partii v roce 1997. Tehdy se jednalo o agenta specializovaného na hraní šachů, tedy nic jiného než hrát šachy nedokázal, byl to nepochybně průlom v umělé inteligenci (AI), ale k obecné umělé inteligenci (AGI) měl daleko. Obecnou umělou inteligencí rozumíme agenta, který dokáže úspěšně vyřešit jakýkoli problém, který dokáže úspěšně vyřešit člověk. První významný krok k vytvoření AGI udělala firma Deepmind v roce 2013, když vytvořila právě algoritmus DQN, jenž dokázal úspěšně hrát různé Atari hry, aniž by se specializoval na každou hru zvlášť. 

Trénování agenta DQN algoritmem má dva problémy, které spolu úzce souvisí. Prvním problémem je relativně pomalé učení, které u Atari her trvá běžně několik dní, druhým problémem poté obtížné nastavení hyperparametrů, kterých je nemalé množství a které naprosto kriticky ovlivňují úspěšnost a dobu učení. Pokud tedy jedno trénování agenta trvá několik dní, je problém najít optimální hyperparametry brute force testováním. Nejvhodnější hodnoty hyperparametrů jsem se tedy rozhodl testovat odlišně. Pomocí toolkitu Open AI Gym testuji hodnoty hyperparametrů na jednoduchých prostředích, které na vstup algoritmu dávají místo snímků obrazovky her vektor čísel přímo popisující stav okolního prostředí a agenta. Tyto úpravy umožní odstranění konvolučních vrstev z neuronové sítě a z několika miliónů parametrů bude mít síť nejednou několik stovek, s čímž se i doba trénování změnší z několika dní na několik jednotek až desítek minut. Toto snížení doby učení mi poté dovolí provézt daleko větší množství testů. Získané poznatky a hodnoty poté aplikuji na prostředí Atari her a výsledky porovnám s těmi, kterých dosáhla firma Deepmind při představení algoritmu.

Algoritmus DQN využívá tři prvky:
prostředí pro trénování
paměť pro uchování odehraných stavů
neuronovou síť, která rozhoduje o tom, která akce bude provedena



K testování hyperparametrů algoritmu jsem si vybral čtveřici prostředí z toolkitu Open AI Gym. Každé prostředí má daný limit počtu kroků, po provedení takovéhoto počtu kroků nastane koncový stav a prostředí musí být zrestarováno. Prostředí na vstupu očekává číslo akce, která má být provedena a na výstup dává vektor čísel popisující stav prostředí (např. rychlost a směr agenta).

Prvním prostředím, které jsem si vybral, je CartPole-v0, agent má zde podobu vozíku s tyčí (balancovník). Vozík se může pohybovat buďto doleva nebo doprava a těmito pohyby musí udržovat tyč v rovnováze, pokud se tyč nakloní o více než 15 stupňů, hra končí. Za každý krok ve kterém agent neztratil rovnováhu dostane odměnu +1, pokud rovnováhu ztratil dostane odměnu -1 a hra končí. Maximální počet kroků agenta v tomto prostředí je 200. Prostředí se poté považuje za vyřešené, pokud bylo agentem dosaženo průměrné skóre minimálně 195 bodů ze 100 epizod.

Druhým prostředím je CartPole-v1, jeho mechanika je stejná jako u CartPole-v0. Dvěma drobnými změnami jsou maximální počet kroků stanovený na 500 a to, že prostředí se považuje za vyřešené od průměrného skóre minimálně 475 bodů ze 100 epizod.

MountainCar-v0 jsem zvolil jako třetí testovací prostředí. Skládá se z údolí a z vozíku, který je na jeho dně. Agent tento vozík kontroluje přičemž má na výběr ze tří možných akcí - jet doleva, jet doprava a nedělat žádný pohyb. Jeho úkolem je vyjet na pravý vrchol údolí. Problém spočívá v tom, že nemá dostatečnou hybnost, aby na vrchol vyjel pouze pomocí pohybu doprava. Musí se nejprve rozjet k levému vrcholu a až poté k pravému, pomocí tohoto manévru získá dostatečnou hybnost pro dosáhnutí pravého vrcholu. Za každý krok ve kterém agent nedosáhl vrcholu dostává odměnu -1, v okamžiku dosažení pravého vrcholu dostává odměnu +1 a hra končí. Maximální počet kroků agenta v tomto prostředí je 200. Prostředí se poté považuje za vyřešené, pokud bylo agentem dosaženo průměrné skóre minimálně -110 bodů ze 100 epizod.

Posledním prostředím, které jsem zvolil pro testování hyperparametrů, je Acrobot-v1. Jedná se o rameno se dvěma klouby, při čemž jeden je pevně ukotvený a druhý je volný. Na začátku je rameno ve svislé poloze. Agentovým úkolem je pohybovat ramenem tak, aby rameno dosáhlo určité výšky, ta je znázorněna čárou nad ramenem. Agent má při plnění cíle na výběr ze tří akcí - působit na pohyblivé rameno směrem doleva, směrem doprava anebo nepůsobit vůbec. Za každý krok ve kterém agent nedosáhl požadované výšky dostává odměnu -1, v okamžiku dosažení dostává odměnu 0 a hra končí. Maximální počet kroků agenta v tomto prostředí je 500. Acrobot-v1 nemá stanovenou výši skóre pro kterou se považuje prostředí za vyřešené. Pro porovnávání úspěšnosti agentů se jako berná mince bere průměrné skóre ze 100 her po 100 epizodách trénování.

Prostředí CartPole-v0 a Cartpole-v1 jsou díky rozdílům v odměnách relativně snadno řešitelná. Je to dáno tím, že jsou od začátku ve stavu maximální odměny a nemusí tento stav hledat, poté v okamžiku ztracení rovnováhy dostanou zápornou odměnou ihned informaci o stavu, který k tomu vedl. Prostředí MountainCar-v0 je přesným opakem, agent náhodně prozkoumává svahy kopce, ale nedostává jinou odměnu než zápornou a tedy žádnou informaci o prostředí. Až po dosažení cíle na vrcholu pravého kopce dostane první informaci o prostředí, kterou může využít pro učení. Než ovšem cíle náhodnými akcemi dosáhne, chvíli to trvá a učení probíhá delší dobu, v některých případech dokonce vrchol vůbec objevit nemusí a učení je poté neúspěšné. V případě prostředí Acrobot-v1 se jedná o zjednoduššenou variantu stejného problému jako v prostředí MountainCar-v0, tedy že dokud agent nedosáhne jiné odměny než záporné, nebude se schopnen nic naučit, pouze v tomto případě je cíl snadněji dosažitelný.

The acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height.



Ramon Llull - https://cs.wikipedia.org/wiki/Ramon_Llull
Umělá inteligence - https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_inteligence

jednoznačný, smysluplný, účelný
https://www.herout.net/
https://www.google.cz/search?q=scientific+writing+lebrun&oq=scientific+writing+lebrun&aqs=chrome..69i57.7721j0j7&sourceid=chrome&ie=UTF-8
vut posudky  [logo vut digital library]
píšou se dva posudky

http://davidvandegrift.com/blog?id=47

\subsection{Kroky algoritmu DQN}
\begin{enumerate}
\item Začni bez použití neuronové sítě se zcela náhodnými akcemi hrát prostředí a ukládej vzpomínku na každý jeho stav do paměti, dokud se paměť nenaplní. Ukládaná vzpomínka se skládá ze stavu prostředí před provedením akce, z provedené akce, ze stavu prostředí po provedení akce, z odměny za nový stav a z informace zdali byl tento stav koncový.
\item Nainicializuj neuronovou síť.
\item Pokud byl dohrán stanovený počet epizod, ukonči učení.
\item Nainicializuj prostředí.
\item Pokud bylo dosaženo maximálního počtu kroků v epizodě, ukonči epizodu a pokračuj bodem 3.
\item S pravděpodobností \textepsilon~proveď náhodnou akci, jinak předej aktuální stav neuronové síti a proveď akci, která bude mít na jejím výstupu nejlepší ohodnocení.
\item Ulož vzpomínku na aktuální stav prostředí.
\item Vezmi náhodný vzorek vzpomínek z paměti.
\item Na vstup neuronové sítě dej stavy prostředí před provedením akcí a její výstup označ jako Q1 (seznam kvality akcí pro každý stav na vstupu), poté dej na vstup neuronové sítě stavy prostředí po provedení akcí a její výstup označ jako Q2 (seznam kvality akcí pro každý stav na vstupu).
\item Uprav kvalitu provedené akce v Q1 pro každou vzpomínku následovně:
\begin{itemize}
\item pokud je stav po provedení akce koncový, nastav kvalitu provedené akce v Q1 na odměnu, kterou agent dostal za její provedení
\item pokud není stav po provedení akce koncový, nastav kvalitu provedené akce v Q1 na odměnu, kterou agent dostal za její provedení a k tomu přičti kvalitu nejlépe ohodnocené akce z Q2 pro danou vzpomínku vynásobenou koeficientem \textgamma.
\end{itemize}
\item Proveď trénování neuronové sítě se stavy prostředí před provedením akcí jako trénovacími daty a s upravenou kvalitou jednotlivých akcí Q1 pro každý stav jako štítky. Vrať se na bod 5.
\end{enumerate}


Odstavec od algoritmu
reinforcement algoritmy
vlastnosti dqn
popsat epsilon a proč je dobré
replay memory
učící konstanta
learning rate problém
nezdůrazňovat testování hyperparametrů
nepsat 4 prostředí pouze prostředí 
proč zrvona tyto jaký aspekt ty prostředí reprezentují nezahrnovat pouze 

tabulka s informacemi o výsledcích
do textu hlubší informaci než číslo
chování algoritmů a variant
v abstraktu popsat poznatky
zjistil jsem že prostředí s touto vlastností se chovají tak a tak

v experimentech nedávat slovo experiment 
místo epsilon dát náhodné akce
vliv
reference
do úvodu nedat použil jsem open ai gym to dát do experimentů


podezřele dlouhé odstavce v úvodu
experiment s obnovou velikost paměti, více častější obnova

V tomto prostředí má agent podobu vozíku s tyčí (balancovník). Vozík se může pohybovat buďto doleva nebo doprava a těmito pohyby musí udržovat tyč v rovnováze, pokud se tyč nakloní o více než 15 stupňů, hra končí. Za každý krok ve kterém agent neztratil rovnováhu dostane odměnu +1, pokud rovnováhu ztratil dostane odměnu -1 a hra končí. Maximální počet kroků agenta v tomto prostředí je 200. Prostředí se poté považuje za vyřešené, pokud bylo agentem dosaženo průměrné skóre minimálně 195 bodů ze 100 epizod.
\subsection{CartPole-v1}
Mechanika prostředí je stejná jako u CartPole-v0. Dvěma drobnými změnami jsou maximální počet kroků stanovený na 500 a to, že prostředí se považuje za vyřešené od průměrného skóre minimálně 475 bodů ze 100 epizod.
\subsection{MountainCar-v0}
Prostředí se skládá z údolí a z vozíku, který je na jeho dně. Agent tento vozík kontroluje přičemž má na výběr ze tří možných akcí - jet doleva, jet doprava a nedělat žádný pohyb. Jeho úkolem je vyjet na pravý vrchol údolí. Problém spočívá v tom, že nemá dostatečnou hybnost, aby na vrchol vyjel pouze pomocí pohybu doprava. Musí se nejprve rozjet k levému vrcholu a až poté k pravému, pomocí tohoto manévru získá dostatečnou hybnost pro dosáhnutí pravého vrcholu. Za každý krok ve kterém agent nedosáhl vrcholu dostává odměnu -1, v okamžiku dosažení pravého vrcholu dostává odměnu +1 a hra končí. Maximální počet kroků agenta v tomto prostředí je 200. Prostředí se poté považuje za vyřešené, pokud bylo agentem dosaženo průměrné skóre minimálně -110 bodů ze 100 epizod.
\subsection{Acrobot-v1}
Agent má podobu ramena se dvěma klouby, při čemž jeden je pevně ukotvený a druhý je volný. Na začátku je rameno ve svislé poloze. Agentovým úkolem je pohybovat ramenem tak, aby rameno dosáhlo určité výšky, ta je znázorněna čárou nad ramenem. Agent má při plnění cíle na výběr ze tří akcí - působit na pohyblivé rameno směrem doleva, směrem doprava anebo nepůsobit vůbec. Za každý krok ve kterém agent nedosáhl požadované výšky dostává odměnu -1, v okamžiku dosažení dostává odměnu 0 a hra končí. Maximální počet kroků agenta v tomto prostředí je 500. Acrobot-v1 nemá stanovenou výši skóre pro kterou se považuje prostředí za vyřešené. Pro porovnávání úspěšnosti agentů se jako berná mince bere průměrné skóre ze 100 her po 100 epizodách trénování.

Pomocí toolkitu Open AI Gym testuji hodnoty hyperparametrů na jednoduchých prostředích, které na vstup algoritmu dávají     