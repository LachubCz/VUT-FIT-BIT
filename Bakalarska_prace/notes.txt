Historická verze
Pojem umělé inteligence je v rámci lidských dějin relativně nový, zavedl ho teprve v roce 1955 John McCarthy. Se samotnou myšlenkou umělé inteligence ale lidstvo koketuje dobu značně delší. Její základ položil ve 13. století Ramon Llull spisem Ars generalis ultima, ve kterém se pokusil pomocí formálních logických prostředků popsat model vytváření znalostí, na němž později demostroval napodobení lidského myšlení. Od poloviny 20. století se tedy jedno odvětví informatiky věnuje umělé inteligenci. Historie umělé inteligence má několik významných milníků, nejznámnějším je pravděpodobně vítězství počítače Deep Blue nad Garri Kasparovem v roce 1997. Jenže tehdy se jednalo o systém specializovaný na hraní šachů, tedy nebyl obecný. První opravdu významný krok k vytvoření obecné umělé inteligence (AGI) udělala firma Deepmind v roce 2013, která vytvořila algoritmus DQN, jenž dokázal úspěšně hrát různé atari hry aniž by se specializoval na každou konkrétně. Právě tomuto algoritmu se ve své práci věnuji a snažím se zlepšit efektivitu jeho učení.

Konečná verze úvodu
Má práce se zabývá algoritmem zpětnovazebného učení DQN, jeho cílem je vytvořit agenta, který se v libovolném prostředí bude schopen naučit inteligentnímu chovaní podle reakcí okolí ve formě odměn, jde tedy o formu umělé inteligence. Historie má několik významných milníků, které jsou spjaty s uvedením agentů, kteří dokázali plnit konkrétní úlohu v konkrétním prostředí. Pravděpodobně nejznámnějším takovým milníkem je vítězství počítače Deep Blue nad Garri Kasparovem v šachové partii v roce 1997. Tehdy se jednalo o agenta specializovaného na hraní šachů, tedy nic jiného než hrát šachy nedokázal, byl to nepochybně průlom v umělé inteligenci (AI), ale k obecné umělé inteligenci (AGI) měl daleko. Obecnou umělou inteligencí rozumíme agenta, který dokáže úspěšně vyřešit jakýkoli problém, který dokáže úspěšně vyřešit člověk. První významný krok k vytvoření AGI udělala firma Deepmind v roce 2013, když vytvořila právě algoritmus DQN, jenž dokázal úspěšně hrát různé Atari hry, aniž by se specializoval na každou hru zvlášť. 

Trénování agenta DQN algoritmem má dva problémy, které spolu úzce souvisí. Prvním problémem je relativně pomalé učení, které u Atari her trvá běžně několik dní, druhým poté obtížné nastavení hyperparametrů, kterých je nemalé množství a které naprosto kriticky ovlivňují míru a dobu učení. Pokud tedy jedno trénování agenta trvá několik dní, je problém najít optimální hyperparametry brute force testováním, nejvhodnější hodnoty hyperparametrů jsem se tedy rozhodl testovat odlišně. Pomocí toolkitu Open AI Gym testuji hodnoty hyperparametrů na jednoduchých prostředích, které na vstup algoritmu dávají místo snímků obrazovky hry vektor čísel přímo popisující stav okolního prostředí a agenta. Tyto úpravy umožní odstranění konvolučních vrstev z neuronové sítě a z několika miliónů parametrů bude mít síť nejednou několik stovek, s čímž se doba trénování změnší z několika dní na několik jednotek až desítek minut. Získané poznatky a hodnoty poté aplikuji na prostředí Atari her a výsledky porovnám s těmi, kterých dosáhla firma Deepmind při představení algoritmu.


K testování hyperparametrů algoritmu jsem si vybral čtveřici prostředí z toolkitu Open AI Gym. Každé prostředí má daný limit počtu kroků, po provedení takovéhoto počtu kroků nastane koncový stav a prostředí musí být zrestarováno. Prostředí na vstupu očekává číslo akce, která má být provedena a na výstup dává vektor čísel popisující stav prostředí (např. rychlost a směr agenta).

Prvním prostředím, které jsem si vybral, je CartPole-v0, agent má zde podobu vozíku s tyčí (balancovník). Vozík se může pohybovat buďto doleva nebo doprava a těmito pohyby musí udržovat tyč v rovnováze, pokud se tyč nakloní o více než 15 stupňů, hra končí. Za každý krok ve kterém agent neztratil rovnováhu dostane odměnu +1, pokud rovnováhu ztratil dostane odměnu -1 a hra končí. Maximální počet kroků agenta v tomto prostředí je 200. Prostředí se poté považuje za vyřešené, pokud bylo agentem dosaženo průměrné skóre minimálně 195 bodů ze 100 epizod.

Druhým prostředím je CartPole-v1, jeho mechanika je stejná jako u CartPole-v0. Dvěma drobnými změnami jsou maximální počet kroků stanovený na 500 a to, že prostředí se považuje za vyřešené od průměrného skóre minimálně 475 bodů ze 100 epizod.

MountainCar-v0 jsem zvolil jako třetí testovací prostředí. Skládá se z údolí a z vozíku, který je na jeho dně. Agent tento vozík kontroluje přičemž má na výběr ze tří možných akcí - jet doleva, jet doprava a nedělat žádný pohyb. Jeho úkolem je vyjet na pravý vrchol údolí. Problém spočívá v tom, že nemá dostatečnou hybnost, aby na vrchol vyjel pouze pomocí pohybu doprava. Musí se nejprve rozjet k levému vrcholu a až poté k pravému, pomocí tohoto manévru získá dostatečnou hybnost pro dosáhnutí pravého vrcholu. Za každý krok ve kterém agent nedosáhl vrcholu dostává odměnu -1, v okamžiku dosažení pravého vrcholu dostává odměnu +1 a hra končí. Maximální počet kroků agenta v tomto prostředí je 200. Prostředí se poté považuje za vyřešené, pokud bylo agentem dosaženo průměrné skóre minimálně -110 bodů ze 100 epizod.

Posledním prostředím, které jsem zvolil pro testování hyperparametrů, je Acrobot-v1. Jedná se o rameno se dvěma klouby, při čemž jeden je pevně ukotvený a druhý je volný. Na začátku je rameno ve svislé poloze. Agentovým úkolem je pohybovat ramenem tak, aby rameno dosáhlo určité výšky, ta je znázorněna čárou nad ramenem. Agent má při plnění cíle na výběr ze tří akcí - působit na pohyblivé rameno směrem doleva, směrem doprava anebo nepůsobit vůbec. Za každý krok ve kterém agent nedosáhl požadované výšky dostává odměnu -1, v okamžiku dosažení dostává odměnu 0 a hra končí. Maximální počet kroků agenta v tomto prostředí je 500. Acrobot-v1 nemá stanovenou výši skóre pro kterou se považuje prostředí za vyřešené. Pro porovnávání úspěšnosti agentů se jako berná mince bere průměrné skóre ze 100 her po 100 epizodách trénování.

Prostředí CartPole-v0 a Cartpole-v1 jsou díky rozdílům v odměnách relativně snadno řešitelná. Je to dáno tím, že jsou od začátku ve stavu maximální odměny a nemusí tento stav hledat, poté v okamžiku ztracení rovnováhy dostanou zápornou odměnou ihned informaci o stavu, který k tomu vedl. Prostředí MountainCar-v0 je přesným opakem, agent náhodně prozkoumává svahy kopce, ale nedostává jinou odměnu než zápornou a tedy žádnou informaci o prostředí. Až po dosažení cíle na vrcholu pravého kopce dostane první informaci o prostředí, kterou může využít pro učení. Než ovšem cíle náhodnými akcemi dosáhne, chvíli to trvá a učení probíhá delší dobu, v některých případech dokonce vrchol vůbec objevit nemusí a učení je poté neúspěšné. V případě prostředí Acrobot-v1 se jedná o zjednoduššenou variantu stejného problému jako v prostředí MountainCar-v0, tedy že dokud agent nedosáhne jiné odměny než záporné, nebude se schopnen nic naučit, pouze v tomto případě je cíl snadněji dosažitelný.

The acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height.



Ramon Llull - https://cs.wikipedia.org/wiki/Ramon_Llull
Umělá inteligence - https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_inteligence

jednoznačný, smysluplný, účelný
https://www.herout.net/
https://www.google.cz/search?q=scientific+writing+lebrun&oq=scientific+writing+lebrun&aqs=chrome..69i57.7721j0j7&sourceid=chrome&ie=UTF-8
vut posudky  [logo vut digital library]
píšou se dva posudky