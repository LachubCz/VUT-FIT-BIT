Historická verze
Pojem umělé inteligence je v rámci lidských dějin relativně nový, zavedl ho teprve v roce 1955 John McCarthy. Se samotnou myšlenkou umělé inteligence ale lidstvo koketuje dobu značně delší. Její základ položil ve 13. století Ramon Llull spisem Ars generalis ultima, ve kterém se pokusil pomocí formálních logických prostředků popsat model vytváření znalostí, na němž později demostroval napodobení lidského myšlení. Od poloviny 20. století se tedy jedno odvětví informatiky věnuje umělé inteligenci. Historie umělé inteligence má několik významných milníků, nejznámnějším je pravděpodobně vítězství počítače Deep Blue nad Garri Kasparovem v roce 1997. Jenže tehdy se jednalo o systém specializovaný na hraní šachů, tedy nebyl obecný. První opravdu významný krok k vytvoření obecné umělé inteligence (AGI) udělala firma Deepmind v roce 2013, která vytvořila algoritmus DQN, jenž dokázal úspěšně hrát různé atari hry aniž by se specializoval na každou konkrétně. Právě tomuto algoritmu se ve své práci věnuji a snažím se zlepšit efektivitu jeho učení.

Konečná verze úvodu
Má práce se zabývá algoritmem zpětnovazebného učení DQN, jeho cílem je vytvořit agenta, který se v libovolném prostředí bude schopen naučit inteligentnímu chovaní podle reakcí okolí ve formě odměn, jde tedy o formu umělé inteligence. Historie má několik významných milníků, které jsou spjaty s uvedením agentů, kteří dokázali plnit konkrétní úlohu v konkrétním prostředí. Pravděpodobně nejznámnějším takovým milníkem je vítězství počítače Deep Blue nad Garri Kasparovem v šachové partii v roce 1997. Tehdy se jednalo o agenta specializovaného na hraní šachů, tedy nic jiného než hrát šachy nedokázal, byl to nepochybně průlom v umělé inteligenci (AI), ale k obecné umělé inteligenci (AGI) měl daleko. Obecnou umělou inteligencí rozumíme agenta, který dokáže úspěšně vyřešit jakýkoli problém, který dokáže úspěšně vyřešit člověk. První významný krok k vytvoření AGI udělala firma Deepmind v roce 2013, když vytvořila právě algoritmus DQN, jenž dokázal úspěšně hrát různé Atari hry, aniž by se specializoval na každou hru zvlášť. 

Trénování agenta DQN algoritmem má dva problémy, které spolu úzce souvisí. Prvním problémem je relativně pomalé učení, které u Atari her trvá běžně několik dní, druhým poté obtížné nastavení hyperparametrů, kterých je nemalé množství a které naprosto kriticky ovlivňují míru a dobu učení. Pokud tedy jedno trénování agenta trvá několik dní, je problém najít optimální hyperparametry brute force testováním, nejvhodnější hodnoty hyperparametrů jsem se tedy rozhodl testovat odlišně. Pomocí toolkitu Open AI Gym testuji hodnoty hyperparametrů na jednoduchých prostředích, které na vstup algoritmu dávají místo snímků obrazovky hry vektor čísel přímo popisující stav okolního prostředí a agenta. Tyto úpravy umožní odstranění konvolučních vrstev z neuronové sítě a z několika miliónů parametrů bude mít síť nejednou několik stovek, s čímž se doba trénování změnší z několika dní na několik jednotek až desítek minut. Získané poznatky a hodnoty poté aplikuji na prostředí Atari her a výsledky porovnám s těmi, kterých dosáhla firma Deepmind při představení algoritmu.


K testování hyperparametrů algoritmu jsem si vybral čtveřici prostředí z toolkitu Open AI Gym. Každé prostředí má určitý limit počtu kroků, po provedení takového počtu kroků nastane koncový stav a prostředí musí být zrestarováno. Prostředí na vstupu očekává akci Každá hra má nastavený maximální počet kroků.

Agent si v prostředí vybírá ze seznamu akcí, které může provést. Na testování hyperparametrů jsem si vybral 4 prostředí z toolkitu Open AI Gym. Prvním prostředím je CartPole-v0, agent má zde podobu vozíku s tyčí (balancovník). Vozík se může pohybovat buďto doleva nebo doprava, těmito pohyby musí udržovat tyč v rovnováze, pokud se tyč nakloní o více než 15 stupňů, hra končí. Za každý krok ve kterém agent neztratil rovnováhu dostane odměnu +1, pokud rovnováhu ztratí dostane odměnu -1. Maximální počet kroků agenta udělaných v tomto prostředí je 200, prostředí se poté považuje za vyřešené, když bylo agentem dosaženo průměrné skóre minimálně 195 za 100 epizod. 

A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.





Ramon Llull - https://cs.wikipedia.org/wiki/Ramon_Llull
Umělá inteligence - https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_inteligence

jednoznačný, smysluplný, účelný
https://www.herout.net/
https://www.google.cz/search?q=scientific+writing+lebrun&oq=scientific+writing+lebrun&aqs=chrome..69i57.7721j0j7&sourceid=chrome&ie=UTF-8
vut posudky  [logo vut digital library]
píšou se dva posudky