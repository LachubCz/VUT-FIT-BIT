{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Gym (https://gym.openai.com) provides us with a lot of different examples and games in which to train a learning agent. The task is to develop one of such agents. We will create a neural network that, given the state of the game (actually, two consecutive states), it outputs a family of quality values (Q-values) for each next possible move. The move with higher Q-value is chosen and performed in the game. This theoretical formalism was taken from https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION: libraries, parameters, network...\n",
    "from keras.models import Sequential      # One layer after the other\n",
    "from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs\n",
    "from collections import deque            # For storing moves \n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/xbucha02/libraries')\n",
    "import gym                                # To train our network\n",
    "from gym import wrappers\n",
    "env = gym.make('MountainCar-v0')          # Choose game (any in the gym should work)\n",
    "env = wrappers.Monitor(env, '/home/lachubcz/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "import random     # For sampling batches from the observations\n",
    "\n",
    "# Create network. Input is two consecutive game states, output is Q-values of the possible moves.\n",
    "model = Sequential()  #vytvoreni linearnho modelu\n",
    "model.add(Dense(20, activation=\"relu\", kernel_initializer=\"uniform\", input_shape=(2, 2)))\n",
    "model.add(Flatten())       # Flatten input so as to have no problems with processing\n",
    "model.add(Dense(18, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(Dense(10, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(Dense(3, activation=\"linear\", kernel_initializer=\"uniform\"))    # Same number of outputs as possible actions\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Parameters\n",
    "                                # Register where the actions will be stored\n",
    "\n",
    "observetime = 2000                         # Number of timesteps we will be acting on the game and observing results\n",
    "epsilon = 0.55                              # Probability of doing a random move\n",
    "epsilonBackup = epsilon\n",
    "gamma = 0.8                                # Discounted future reward. How much we care about steps further in time\n",
    "mb_size = 50                               # Learning minibatch size\n",
    "episodes = 5000                               # Pocet epizod\n",
    "numberOfGames = 20\n",
    "observation = env.reset()                     # Game begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0. Episode Finished\n",
      "0\n",
      "1. Episode Finished\n",
      "0\n",
      "2. Episode Finished\n",
      "0\n",
      "3. Episode Finished\n",
      "0\n",
      "4. Episode Finished\n",
      "0\n",
      "5. Episode Finished\n",
      "0\n",
      "6. Episode Finished\n",
      "0\n",
      "7. Episode Finished\n",
      "0\n",
      "8. Episode Finished\n",
      "0\n",
      "9. Episode Finished\n",
      "0\n",
      "10. Episode Finished\n",
      "0\n",
      "11. Episode Finished\n",
      "0\n",
      "12. Episode Finished\n",
      "0\n",
      "13. Episode Finished\n",
      "0\n",
      "14. Episode Finished\n",
      "0\n",
      "15. Episode Finished\n",
      "0\n",
      "16. Episode Finished\n",
      "0\n",
      "17. Episode Finished\n",
      "0\n",
      "18. Episode Finished\n",
      "0\n",
      "19. Episode Finished\n",
      "0\n",
      "20. Episode Finished\n",
      "0\n",
      "21. Episode Finished\n",
      "0\n",
      "22. Episode Finished\n",
      "0\n",
      "23. Episode Finished\n",
      "0\n",
      "24. Episode Finished\n",
      "0\n",
      "25. Episode Finished\n",
      "0\n",
      "26. Episode Finished\n",
      "0\n",
      "27. Episode Finished\n",
      "0\n",
      "28. Episode Finished\n",
      "0\n",
      "29. Episode Finished\n",
      "0\n",
      "30. Episode Finished\n",
      "0\n",
      "31. Episode Finished\n",
      "0\n",
      "32. Episode Finished\n",
      "0\n",
      "33. Episode Finished\n",
      "0\n",
      "34. Episode Finished\n",
      "0\n",
      "35. Episode Finished\n",
      "0\n",
      "36. Episode Finished\n",
      "0\n",
      "37. Episode Finished\n",
      "0\n",
      "38. Episode Finished\n",
      "0\n",
      "39. Episode Finished\n",
      "0\n",
      "40. Episode Finished\n",
      "0\n",
      "41. Episode Finished\n",
      "0\n",
      "42. Episode Finished\n",
      "0\n",
      "43. Episode Finished\n",
      "0\n",
      "44. Episode Finished\n",
      "0\n",
      "45. Episode Finished\n",
      "0\n",
      "46. Episode Finished\n",
      "0\n",
      "47. Episode Finished\n",
      "0\n",
      "48. Episode Finished\n",
      "0\n",
      "49. Episode Finished\n",
      "0\n",
      "50. Episode Finished\n",
      "0\n",
      "51. Episode Finished\n",
      "0\n",
      "52. Episode Finished\n",
      "0\n",
      "53. Episode Finished\n",
      "0\n",
      "54. Episode Finished\n",
      "0\n",
      "55. Episode Finished\n",
      "0\n",
      "56. Episode Finished\n",
      "0\n",
      "57. Episode Finished\n",
      "0\n",
      "58. Episode Finished\n",
      "0\n",
      "59. Episode Finished\n",
      "0\n",
      "60. Episode Finished\n",
      "0\n",
      "61. Episode Finished\n",
      "0\n",
      "62. Episode Finished\n",
      "0\n",
      "63. Episode Finished\n",
      "0\n",
      "64. Episode Finished\n",
      "0\n",
      "65. Episode Finished\n",
      "0\n",
      "66. Episode Finished\n",
      "0\n",
      "67. Episode Finished\n",
      "0\n",
      "68. Episode Finished\n",
      "0\n",
      "69. Episode Finished\n",
      "0\n",
      "70. Episode Finished\n",
      "0\n",
      "71. Episode Finished\n",
      "0\n",
      "72. Episode Finished\n",
      "0\n",
      "73. Episode Finished\n",
      "0\n",
      "74. Episode Finished\n",
      "0\n",
      "75. Episode Finished\n",
      "0\n",
      "76. Episode Finished\n",
      "0\n",
      "77. Episode Finished\n",
      "0\n",
      "78. Episode Finished\n",
      "0\n",
      "79. Episode Finished\n",
      "0\n",
      "80. Episode Finished\n",
      "0\n",
      "81. Episode Finished\n",
      "0\n",
      "82. Episode Finished\n",
      "0\n",
      "83. Episode Finished\n",
      "0\n",
      "84. Episode Finished\n",
      "0\n",
      "85. Episode Finished\n",
      "0\n",
      "86. Episode Finished\n",
      "0\n",
      "87. Episode Finished\n",
      "0\n",
      "88. Episode Finished\n",
      "0\n",
      "89. Episode Finished\n",
      "0\n",
      "90. Episode Finished\n",
      "0\n",
      "91. Episode Finished\n",
      "0\n",
      "92. Episode Finished\n",
      "0\n",
      "93. Episode Finished\n",
      "0\n",
      "94. Episode Finished\n",
      "0\n",
      "95. Episode Finished\n",
      "0\n",
      "96. Episode Finished\n",
      "0\n",
      "97. Episode Finished\n",
      "0\n",
      "98. Episode Finished\n",
      "0\n",
      "99. Episode Finished\n",
      "0\n",
      "100. Episode Finished\n",
      "0\n",
      "101. Episode Finished\n",
      "0\n",
      "102. Episode Finished\n",
      "0\n",
      "103. Episode Finished\n",
      "0\n",
      "104. Episode Finished\n",
      "0\n",
      "105. Episode Finished\n",
      "0\n",
      "106. Episode Finished\n",
      "0\n",
      "107. Episode Finished\n",
      "0\n",
      "108. Episode Finished\n",
      "0\n",
      "109. Episode Finished\n",
      "0\n",
      "110. Episode Finished\n",
      "0\n",
      "111. Episode Finished\n",
      "0\n",
      "112. Episode Finished\n",
      "0\n",
      "113. Episode Finished\n",
      "0\n",
      "114. Episode Finished\n",
      "0\n",
      "115. Episode Finished\n",
      "0\n",
      "116. Episode Finished\n",
      "0\n",
      "117. Episode Finished\n",
      "0\n",
      "118. Episode Finished\n",
      "0\n",
      "119. Episode Finished\n",
      "0\n",
      "120. Episode Finished\n",
      "0\n",
      "121. Episode Finished\n",
      "0\n",
      "122. Episode Finished\n",
      "0\n",
      "123. Episode Finished\n",
      "0\n",
      "124. Episode Finished\n",
      "0\n",
      "125. Episode Finished\n",
      "0\n",
      "126. Episode Finished\n",
      "0\n",
      "127. Episode Finished\n",
      "0\n",
      "128. Episode Finished\n",
      "0\n",
      "129. Episode Finished\n",
      "0\n",
      "130. Episode Finished\n",
      "0\n",
      "131. Episode Finished\n",
      "0\n",
      "132. Episode Finished\n",
      "0\n",
      "133. Episode Finished\n",
      "0\n",
      "134. Episode Finished\n",
      "0\n",
      "135. Episode Finished\n",
      "0\n",
      "136. Episode Finished\n",
      "0\n",
      "137. Episode Finished\n",
      "0\n",
      "138. Episode Finished\n",
      "0\n",
      "139. Episode Finished\n",
      "0\n",
      "140. Episode Finished\n",
      "0\n",
      "141. Episode Finished\n",
      "0\n",
      "142. Episode Finished\n",
      "0\n",
      "143. Episode Finished\n",
      "0\n",
      "144. Episode Finished\n",
      "0\n",
      "145. Episode Finished\n",
      "0\n",
      "146. Episode Finished\n",
      "0\n",
      "147. Episode Finished\n",
      "0\n",
      "148. Episode Finished\n",
      "0\n",
      "149. Episode Finished\n",
      "0\n",
      "150. Episode Finished\n",
      "0\n",
      "151. Episode Finished\n",
      "0\n",
      "152. Episode Finished\n",
      "0\n",
      "153. Episode Finished\n",
      "0\n",
      "154. Episode Finished\n",
      "0\n",
      "155. Episode Finished\n",
      "0\n",
      "156. Episode Finished\n",
      "0\n",
      "157. Episode Finished\n",
      "0\n",
      "158. Episode Finished\n",
      "0\n",
      "159. Episode Finished\n",
      "0\n",
      "160. Episode Finished\n",
      "0\n",
      "161. Episode Finished\n",
      "0\n",
      "162. Episode Finished\n",
      "0\n",
      "163. Episode Finished\n",
      "0\n",
      "164. Episode Finished\n",
      "0\n",
      "165. Episode Finished\n",
      "0\n",
      "166. Episode Finished\n",
      "0\n",
      "167. Episode Finished\n",
      "0\n",
      "168. Episode Finished\n",
      "0\n",
      "169. Episode Finished\n",
      "0\n",
      "170. Episode Finished\n",
      "0\n",
      "171. Episode Finished\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# FIRST STEP: Knowing what each action does (Observing)\n",
    "for eps in range (episodes):\n",
    "    \n",
    "    \n",
    "    env.render()\n",
    "    D = deque() #vyprazdneni D\n",
    "    print('{}' .format(len(D)))\n",
    "    #if D:\n",
    "    #    print('not gut')\n",
    "    \n",
    "    obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
    "    state = np.stack((obs, obs), axis=1)\n",
    "    done = False\n",
    "\n",
    "    for t in range(observetime):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
    "        else:\n",
    "            Q = model.predict(state)          # Q-values predictions\n",
    "            action = np.argmax(Q)             # Move with highest Q-value is the chosen one\n",
    "            \n",
    "        observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action\n",
    "        if reward != -1.0:\n",
    "            print('{}. Uspech!'.format(t)) #dosazeni cile\n",
    "        obs_new = np.expand_dims(observation_new, axis=0)          # (Formatting issues)\n",
    "        state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     # Update the input with the new state of the game\n",
    "        D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence\n",
    "        state = state_new         # Update state\n",
    "\n",
    "        #print('{}'.format(epsilon))\n",
    "        if done:\n",
    "            env.reset()           # Restart game if it's finished\n",
    "            env.render()\n",
    "            obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs \n",
    "            state = np.stack((obs, obs), axis=1)\n",
    "            \n",
    "    epsilon -= (epsilonBackup - 0.1) * (1/episodes)\n",
    "    #print('{}. Observing Finished' .format())\n",
    "\n",
    "    # SECOND STEP: Learning from the observations (Experience replay)\n",
    "    minibatch = random.sample(D, mb_size) #z D vybere pocet mb_size samplu\n",
    "\n",
    "    inputs_shape = (mb_size,) + state.shape[1:]\n",
    "    inputs = np.zeros(inputs_shape)\n",
    "    targets = np.zeros((mb_size, env.action_space.n)) #vytvori pole ([], []), 1. argument - radky, 2. argument - sloupce\n",
    "\n",
    "    for i in range(0, mb_size):\n",
    "        state = minibatch[i][0]\n",
    "        action = minibatch[i][1]\n",
    "        reward = minibatch[i][2]\n",
    "        state_new = minibatch[i][3]\n",
    "        done = minibatch[i][4]\n",
    "\n",
    "        # Build Bellman equation for the Q function\n",
    "        inputs[i:i+1] = np.expand_dims(state, axis=0)\n",
    "        targets[i] = model.predict(state)\n",
    "        Q_sa = model.predict(state_new)\n",
    "\n",
    "        if done:\n",
    "            targets[i, action] = reward\n",
    "        else:\n",
    "            targets[i, action] = reward + gamma * np.max(Q_sa)\n",
    "            #print('{}'.format(Q_sa))\n",
    "\n",
    "    \n",
    "    model.train_on_batch(inputs, targets) # Train network to output the Q function, mozna neni casti cyklu\n",
    "    #print('Inputs: {}; Targets: {}'.format(inputs, targets))\n",
    "    print('{}. Episode Finished'.format(eps))\n",
    "    eps+= 1 #aktualizovani poctu epizod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD STEP: Play!\n",
    "for x in range(0, numberOfGames):\n",
    "    \n",
    "    obs = np.expand_dims(observation, axis=0)\n",
    "    state = np.stack((obs, obs), axis=1)\n",
    "    done = False\n",
    "    tot_reward = 0.0\n",
    "    while not done:\n",
    "        env.render()                    # Uncomment to see game running\n",
    "        #print('State: {}'.format(state))\n",
    "        Q = model.predict(state)\n",
    "        #print('Q: {}'.format(Q))\n",
    "        action = np.argmax(Q)  \n",
    "        print('Action: {}'.format(action))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        obs = np.expand_dims(observation, axis=0)\n",
    "        state = np.append(np.expand_dims(obs, axis=0), state[:, :1, :], axis=1)    \n",
    "        tot_reward += reward\n",
    "    observation = env.reset()\n",
    "    print('{}. Game ended! Total reward: {}'.format(x, tot_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
